{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a26710-4653-4de8-a13a-816da156438b",
   "metadata": {},
   "source": [
    "webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a85db1-e495-417c-9a85-5411e6d6fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import serial\n",
    "import time\n",
    "\n",
    "# Set up serial communication with Arduino\n",
    "arduino = serial.Serial('COM7', 9600)  # Change 'COM3' to your Arduino port\n",
    "time.sleep(2)  # Wait for the connection to establish\n",
    "\n",
    "# Load the face detector\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Analyze frame for emotion\n",
    "    result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "    # Draw rectangles and emotions\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Get the dominant emotion\n",
    "        emotion = result[0]['dominant_emotion']\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            emotion,\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "            cv2.LINE_4\n",
    "        )\n",
    "\n",
    "        # Check if the emotion is 'sad'\n",
    "        if emotion == 'sad':\n",
    "            arduino.write(b'0')  # Send signal to Arduino to activate mist maker\n",
    "        else:\n",
    "            arduino.write(b'1')  # Send signal to Arduino to deactivate mist maker\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Demo video', frame)\n",
    "\n",
    "    # Quit on pressing 'q'\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "arduino.close()  # Close the serial connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71794a-a788-4e18-905a-7b7b663a249d",
   "metadata": {},
   "source": [
    "webcam with music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a3fee-bff5-4042-b55c-de538cd0fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import pygame  # For music playback\n",
    "import serial  # For Arduino communication\n",
    "import time\n",
    "\n",
    "# Set up serial communication with Arduino\n",
    "arduino = serial.Serial('COM7', 9600)  # Change 'COM7' to match your Arduino port\n",
    "time.sleep(2)  # Wait for the connection to establish\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load the face detector\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Function for gamma correction\n",
    "def apply_gamma_correction(image, gamma=1.5):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "# Function to enhance exposure and contrast\n",
    "def enhance_frame(frame):\n",
    "    frame = apply_gamma_correction(frame, gamma=1.5)\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    enhanced_frame = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_frame\n",
    "\n",
    "# Play music based on emotion\n",
    "current_emotion = None\n",
    "music_tracks = {\n",
    "    \"happy\": r\"G:\\ad\\music\\happy.mp3\",\n",
    "    \"sad\": r\"G:\\ad\\music\\sad.mp3\",\n",
    "}\n",
    "\n",
    "# Function to play music based on emotion\n",
    "def play_music(emotion):\n",
    "    global current_emotion\n",
    "\n",
    "    if emotion in music_tracks:\n",
    "        if current_emotion != emotion:\n",
    "            current_emotion = emotion\n",
    "            pygame.mixer.music.stop()\n",
    "            pygame.mixer.music.load(music_tracks[emotion])\n",
    "            pygame.mixer.music.play()\n",
    "            # Send emotion to Arduino\n",
    "            arduino.write((emotion + \"\\n\").encode())\n",
    "    else:\n",
    "        pygame.mixer.music.stop()\n",
    "        arduino.write(b\"none\\n\")\n",
    "\n",
    "def analyze_emotion(frame):\n",
    "    try:\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        return result[0]['dominant_emotion']\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main loop\n",
    "frame_count = 0\n",
    "emotion = None  # Initialize here to prevent undefined error\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = enhance_frame(frame)\n",
    "\n",
    "    # Analyze emotion every 5 frames\n",
    "    if frame_count % 5 == 0:\n",
    "        detected_emotion = analyze_emotion(frame)\n",
    "        if detected_emotion:\n",
    "            emotion = detected_emotion\n",
    "            play_music(emotion)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            emotion if emotion else \"No Emotion\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "            cv2.LINE_4\n",
    "        )\n",
    "\n",
    "    cv2.imshow('Emotion-based Music Player', frame)\n",
    "\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "arduino.close()  # Close the serial connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d7f91-1da4-425c-ab06-109d05839721",
   "metadata": {},
   "source": [
    "To run your **emotion-based music player with Arduino integration**, you'll need to install several **Python libraries**, ensure **hardware setup**, and have some **media files** and **Arduino code** ready. Here's a full list of what you need to install and prepare:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ PYTHON LIBRARIES TO INSTALL\n",
    "\n",
    "You can install them via `pip`:\n",
    "\n",
    "```bash\n",
    "pip install opencv-python deepface numpy pygame pyserial\n",
    "```\n",
    "\n",
    "**Explanation of each:**\n",
    "\n",
    "| Library       | Purpose                                           |\n",
    "|---------------|---------------------------------------------------|\n",
    "| `opencv-python` | Video capture and face detection               |\n",
    "| `deepface`      | Emotion detection using deep learning          |\n",
    "| `numpy`         | Image manipulation (gamma correction, etc.)    |\n",
    "| `pygame`        | Music playback                                 |\n",
    "| `pyserial`      | Communication with Arduino via serial          |\n",
    "\n",
    "**Note:** `deepface` may require additional dependencies (like `tensorflow`, `keras`, `mtcnn`). They should install automatically with DeepFace, but if not, you can install manually:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow keras mtcnn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ SYSTEM REQUIREMENTS\n",
    "\n",
    "1. **Python 3.7–3.11** (recommended)\n",
    "2. **Camera** (webcam)\n",
    "3. **Speakers** (for audio playback)\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ HARDWARE SETUP\n",
    "\n",
    "- **Arduino board** (Uno, Nano, etc.)\n",
    "- Connect Arduino to your PC via USB\n",
    "- Make sure your Arduino is programmed to respond to serial inputs like `\"happy\\n\"` or `\"sad\\n\"` — this can control LEDs or motors, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ MUSIC FILES\n",
    "\n",
    "Put the correct music files in the specified paths:\n",
    "```python\n",
    "music_tracks = {\n",
    "    \"happy\": r\"G:\\ad\\music\\happy.mp3\",\n",
    "    \"sad\": r\"G:\\ad\\music\\sad.mp3\",\n",
    "}\n",
    "```\n",
    "Make sure these files exist, or change the paths accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ ARDUINO SIDE CODE (Example)\n",
    "\n",
    "Upload this via Arduino IDE:\n",
    "\n",
    "```cpp\n",
    "void setup() {\n",
    "  Serial.begin(9600);\n",
    "  pinMode(13, OUTPUT);  // Example pin\n",
    "}\n",
    "\n",
    "void loop() {\n",
    "  if (Serial.available()) {\n",
    "    String emotion = Serial.readStringUntil('\\n');\n",
    "    if (emotion == \"happy\") {\n",
    "      digitalWrite(13, HIGH); // Happy mode\n",
    "    } else if (emotion == \"sad\") {\n",
    "      digitalWrite(13, LOW);  // Sad mode\n",
    "    } else {\n",
    "      digitalWrite(13, LOW);  // Default\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ OPTIONAL: ENVIRONMENT SETUP\n",
    "\n",
    "- **Virtual environment** (recommended to avoid version conflicts):\n",
    "```bash\n",
    "python -m venv emo_env\n",
    "emo_env\\Scripts\\activate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ SUMMARY CHECKLIST\n",
    "\n",
    "| ✅ Item                      | Description                          |\n",
    "|-----------------------------|--------------------------------------|\n",
    "| Python + pip                | Installed on your system             |\n",
    "| Required libraries          | Installed via `pip`                  |\n",
    "| Webcam                      | Working and accessible               |\n",
    "| Arduino                     | Connected and code uploaded          |\n",
    "| Music files                 | In correct paths                     |\n",
    "| Arduino serial port         | Matches `'COM7'` or change if needed |\n",
    "| Proper permissions          | To access camera & audio             |\n",
    "\n",
    "---\n",
    "\n",
    "Want me to create a simple interface to test it or generate Arduino code based on multiple emotions (like angry, neutral, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cad99-84a9-44cc-b015-dba3969618dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571776ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Arduino Code: Emotion-based Mist Maker Control\n",
    "\n",
    "#define RELAY_HAPPY 7  // Relay 1 - Mist Maker 1\n",
    "#define RELAY_SAD 8    // Relay 2 - Mist Maker 2\n",
    "\n",
    "String inputString = \"\";\n",
    "\n",
    "void setup() {\n",
    "  pinMode(RELAY_HAPPY, OUTPUT);\n",
    "  pinMode(RELAY_SAD, OUTPUT);\n",
    "  \n",
    "  // Start with both mist makers OFF\n",
    "  digitalWrite(RELAY_HAPPY, LOW);\n",
    "  digitalWrite(RELAY_SAD, LOW);\n",
    "\n",
    "  Serial.begin(9600);\n",
    "}\n",
    "\n",
    "void loop() {\n",
    "  while (Serial.available()) {\n",
    "    char c = Serial.read();\n",
    "    if (c == '\\n') {\n",
    "      inputString.trim();\n",
    "\n",
    "      if (inputString == \"happy\") {\n",
    "        digitalWrite(RELAY_HAPPY, HIGH); // Turn ON happy mist\n",
    "        digitalWrite(RELAY_SAD, LOW);    // Turn OFF sad mist\n",
    "      } \n",
    "      else if (inputString == \"sad\") {\n",
    "        digitalWrite(RELAY_HAPPY, LOW);\n",
    "        digitalWrite(RELAY_SAD, HIGH);\n",
    "      } \n",
    "      else {\n",
    "        digitalWrite(RELAY_HAPPY, LOW);\n",
    "        digitalWrite(RELAY_SAD, LOW);\n",
    "      }\n",
    "\n",
    "      inputString = \"\";  // Clear buffer\n",
    "    } else {\n",
    "      inputString += c;\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a193585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lalan\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "import pygame\n",
    "import serial\n",
    "import time\n",
    "\n",
    "# Connect to Arduino (change 'COM7' as needed)\n",
    "arduino = serial.Serial('COM7', 9600)\n",
    "time.sleep(2)\n",
    "\n",
    "# Initialize pygame for music\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load face detector\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "def apply_gamma_correction(image, gamma=1.5):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def enhance_frame(frame):\n",
    "    frame = apply_gamma_correction(frame)\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "music_tracks = {\n",
    "    \"happy\": r\"D:\\ad\\music\\happy.mp3\",\n",
    "    \"sad\": r\"D:\\ad\\music\\sad.mp3\",\n",
    "}\n",
    "\n",
    "current_emotion = None\n",
    "\n",
    "def play_music_and_trigger_relay(emotion):\n",
    "    global current_emotion\n",
    "    if emotion in music_tracks:\n",
    "        if current_emotion != emotion:\n",
    "            current_emotion = emotion\n",
    "            pygame.mixer.music.stop()\n",
    "            pygame.mixer.music.load(music_tracks[emotion])\n",
    "            pygame.mixer.music.play()\n",
    "            arduino.write((emotion + \"\\n\").encode())\n",
    "    else:\n",
    "        pygame.mixer.music.stop()\n",
    "        arduino.write(b\"none\\n\")\n",
    "\n",
    "def analyze_emotion(frame):\n",
    "    try:\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        return result[0]['dominant_emotion']\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis error: {e}\")\n",
    "        return None\n",
    "\n",
    "frame_count = 0\n",
    "emotion = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = enhance_frame(frame)\n",
    "\n",
    "    if frame_count % 50 == 0:\n",
    "        detected_emotion = analyze_emotion(frame)\n",
    "        if detected_emotion:\n",
    "            if detected_emotion in ['happy', 'sad']:\n",
    "                emotion = detected_emotion\n",
    "                play_music_and_trigger_relay(emotion)\n",
    "            else:\n",
    "                arduino.write(b\"none\\n\")\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            emotion if emotion else \"No Emotion\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 0, 255),\n",
    "            2\n",
    "        )\n",
    "\n",
    "    cv2.imshow('Emotion-based Mist Maker Control', frame)\n",
    "\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "arduino.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d785f65-7c46-4415-90cb-0bf50f8fa162",
   "metadata": {},
   "source": [
    "<h1>with custum model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341717c-6694-48a4-87b5-bccf2b1cef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pygame  # For music playback\n",
    "import serial\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Connect to Arduino (change 'COM7' as needed)\n",
    "arduino = serial.Serial('COM7', 9600)\n",
    "time.sleep(2)\n",
    "\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model(r\"D:\\ad\\emotion_model_datset.h5\")\n",
    "\n",
    "# Define class labels based on your training\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load the face detector\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "# Function for gamma correction\n",
    "def apply_gamma_correction(image, gamma=1.5):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "# Function to enhance exposure and contrast\n",
    "def enhance_frame(frame):\n",
    "    frame = apply_gamma_correction(frame, gamma=1.5)\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    enhanced_frame = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    return enhanced_frame\n",
    "\n",
    "# Play music based on emotion\n",
    "current_emotion = None\n",
    "music_tracks = {\n",
    "    \"happy\": r\"D:\\ad\\music\\happy.mp3\",\n",
    "    \"sad\": r\"D:\\ad\\music\\sad.mp3\"\n",
    "}\n",
    "\n",
    "# Function to play music based on emotion\n",
    "def play_music(emotion):\n",
    "    global current_emotion\n",
    "    if emotion in music_tracks:\n",
    "        if current_emotion != emotion:\n",
    "            current_emotion = emotion\n",
    "            pygame.mixer.music.stop()\n",
    "            pygame.mixer.music.load(music_tracks[emotion])\n",
    "            pygame.mixer.music.play()\n",
    "    else:\n",
    "        pygame.mixer.music.stop()\n",
    "\n",
    "# Analyze emotion using your custom model\n",
    "def analyze_emotion_custom(face_img):\n",
    "    try:\n",
    "        face_img = cv2.resize(face_img, (48, 48))\n",
    "        face_img = face_img.astype(\"float\") / 255.0\n",
    "        face_img = img_to_array(face_img)\n",
    "        face_img = np.expand_dims(face_img, axis=0)\n",
    "        preds = model.predict(face_img, verbose=0)[0]\n",
    "        return emotion_labels[np.argmax(preds)]\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main loop\n",
    "frame_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = enhance_frame(frame)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "    detected_emotion = None\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        detected_emotion = analyze_emotion_custom(roi_gray)\n",
    "\n",
    "        if frame_count % 20 == 0 and detected_emotion:\n",
    "            play_music(detected_emotion)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            detected_emotion if detected_emotion else \"No Emotion\",\n",
    "            (x, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "            cv2.LINE_4\n",
    "        )\n",
    "\n",
    "    cv2.imshow('Emotion-based Music Player', frame)\n",
    "\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "arduino.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec0be78-a880-49ed-a1e6-add5369e79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserial in c:\\users\\lalan\\anaconda3\\lib\\site-packages (3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyserial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd84753-41d3-422e-91cc-408facd94b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
